## RNN循环神经网络
注意：由于RNN循环神经网络是针对有序列数据的算法，在进行图片数据训练时输入数据的形式要求会与CNN要求不同，强烈建议学习完图片训练后，学习文本训练，会加强对算法的理解
### 手写数字图片训练过程
1. reshape 三维数据转换成二维数据
[None,time_steps,n_inputs]-->[None,n_inputs]
2. 设置cell,tf.contrib.rnn.BasicLSTMCell
3. tf.nn.dynamic_rnn
4. 计算精度，并重复训练
```python
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
#导入数据
mnist=input_data.read_data_sets('MNIST_data',one_hot=True)

#超参数设置
learning_rate=0.001
training_steps=100000
batch_size=128
n_inputs=28
time_steps=28
hidden_units=128
n_classes=10


x=tf.placeholder(tf.float32,[None,time_steps,n_inputs])
y=tf.placeholder(tf.float32,[None,n_classes])

weights={
    'in-hidden':tf.Variable(tf.random_normal([n_inputs,hidden_units])),
    'hidden-out':tf.Variable(tf.random_normal([hidden_units,n_classes]))
}

biases={
    'in-hidden':tf.Variable(tf.constant(0.1,shape=[hidden_units,])),
    'hidden-out':tf.Variable(tf.constant(0.1,shape=[n_classes,]))
}

def RNN(X,weights,biases):
    # 原始的 X 是 3 维数据, 我们需要把它变成 2 维数据才能使用 weights 的矩阵乘法
    # X ==> (128 batches * 28 steps, 28 inputs)
    X=tf.reshape(X,[-1,n_inputs])

    # X_in = W*X + b
    X_in=tf.matmul(X,weights['in-hidden'])+biases['in-hidden']
    # X_in ==> (128 batches, 28 steps, 128 hidden) 换回3维
    X_in = tf.reshape(X_in, [-1, time_steps, hidden_units])
    # 使用 basic LSTM Cell.
    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_units, forget_bias=1.0, state_is_tuple=True)
    init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)  # 初始化全零 state
    outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, X_in, initial_state=init_state, time_major=False)
    results = tf.matmul(final_state[1], weights['hidden-out']) + biases['hidden-out']
    return results

pred=RNN(x,weights,biases)
cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))
train_op=tf.train.AdamOptimizer(learning_rate).minimize(cost)

correct_pred=tf.equal(tf.argmax(pred,1),tf.argmax(y,1))
accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))

init=tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    for step in range(training_steps):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        batch_xs = batch_xs.reshape([batch_size, time_steps, n_inputs])
        sess.run(train_op,feed_dict={
            x:batch_xs,
            y:batch_ys
        })
        if step % 50==0:
            print(sess.run(accuracy,feed_dict={
                x:batch_xs,
                y:batch_ys
            }))






```

### 参考资料
- [RNN 循环神经网络](https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-07-RNN1/)
- [理解LSTM网络](http://www.jeyzhang.com/understanding-lstm-network.html)
- [full_code.py](https://github.com/MorvanZhou/tutorials/blob/master/tensorflowTUT/tf20_RNN2/full_code.py)
- [零基础入门深度学习(6) - 长短时记忆网络(LSTM)](https://zybuluo.com/hanbingtao/note/581764)
- [Recurrent Neural Networks](https://www.tensorflow.org/tutorials/recurrent)